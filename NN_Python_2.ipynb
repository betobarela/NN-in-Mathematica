{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d06d2ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d98190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multilayer_Perceptron:\n",
    "    \n",
    "    # `layer_arch` gives the number of nodes in every layer starting from the second one\n",
    "    # `layer_arch` does not include the bias node, i.e., if layer_arch[layer] = 2, than \n",
    "    # that layer has 2 + 1(bias) = 3 layers\n",
    "    def __init__(self, dataset, target, layer_arch, activation_function = (lambda x: 1 / (1 + np.exp(-x))), \n",
    "                 gradient_function = (lambda x: x*(1-x))):\n",
    "        self.dataset = dataset\n",
    "        # below we define the bias column\n",
    "        self.dataset = [instance + [1] for instance in self.dataset]\n",
    "        self.layer_arch = layer_arch        \n",
    "        self.target = target\n",
    "        self.activation_function = activation_function\n",
    "        self.gradient_function = gradient_function   \n",
    "                    \n",
    "    def update_outs(self):\n",
    "        for instance_index in np.arange(len(self.mini_batch)):\n",
    "            for layer_index in np.arange(len(self.layer_arch)):\n",
    "                layer_index = layer_index + 1\n",
    "                weight = self.weights[layer_index-1].T\n",
    "                out = self.outs[instance_index][layer_index-1]\n",
    "                new_out_array = np.array(list(map(self.activation_function, weight @ out)))\n",
    "                self.outs[instance_index][layer_index][:len(new_out_array)] = new_out_array\n",
    "                \n",
    "            self.deltas[instance_index][-1] = (self.mini_target[instance_index] - self.outs[instance_index][-1])\\\n",
    "            *np.array(list(map(self.gradient_function, self.outs[instance_index][-1])))\n",
    "            \n",
    "            for layer_inv_index in np.arange(len(self.layer_arch)-1):\n",
    "                layer_inv_index += 1          \n",
    "                weight = self.weights[-layer_inv_index]\n",
    "                delta = self.deltas[instance_index][-layer_inv_index]\n",
    "                if layer_inv_index > 1:\n",
    "                    delta = self.deltas[instance_index][-layer_inv_index][:-1]\n",
    "                out = self.outs[instance_index][-(layer_inv_index+1)]  \n",
    "                delta_new_array = np.array(list(map(self.gradient_function, out)))*(weight @ delta) \n",
    "                self.deltas[instance_index][-(layer_inv_index + 1)] = delta_new_array\n",
    "                \n",
    "    def update_weights(self, momentum = 1, learning_rate = 0.5):\n",
    "        for instance_index in np.arange(len(self.mini_batch)):\n",
    "            for layer_inv_index in np.arange(len(self.layer_arch)):\n",
    "                layer_inv_index += 1\n",
    "                weight_original = self.weights[-layer_inv_index]\n",
    "                delta = self.deltas[instance_index][-layer_inv_index]\n",
    "                if layer_inv_index > 1:\n",
    "                    delta = self.deltas[instance_index][-layer_inv_index][:-1]\n",
    "                out = self.outs[instance_index][-(layer_inv_index+1)]\n",
    "                increment = np.outer(out, delta)\n",
    "                self.weights[-layer_inv_index] = momentum*weight_original + learning_rate*increment\n",
    "                   \n",
    "    def fit(self, epochs=100, batch_size=32, learning_rate=0.5, momentum=1, print_training_predictions=False):\n",
    "        num_instances = len(self.dataset)\n",
    "        counter = 0\n",
    "\n",
    "        self.weights = [np.random.uniform(-1, 1, [len(self.dataset[0]), self.layer_arch[0]])]\n",
    "        for index in np.arange(len(self.layer_arch))[:-1]:\n",
    "            self.weights.append(np.random.uniform(-1, 1, [self.layer_arch[index]+1, self.layer_arch[index+1]]))\n",
    "        \n",
    "        while counter < epochs:\n",
    "            # Shuffle the dataset and targets\n",
    "            indices = np.arange(num_instances)\n",
    "            np.random.shuffle(indices)\n",
    "            shuffled_dataset = [self.dataset[i] for i in indices]\n",
    "            shuffled_target = [self.target[i] for i in indices]\n",
    "                \n",
    "            for start_idx in range(0, num_instances, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, num_instances)\n",
    "                self.mini_batch = shuffled_dataset[start_idx:end_idx]\n",
    "                self.mini_target = shuffled_target[start_idx:end_idx]\n",
    "\n",
    "                self.outs = [[instance] for instance in self.mini_batch]\n",
    "                for instance_index in np.arange(len(self.mini_batch)):  \n",
    "                    for index in np.arange(len(self.layer_arch)):\n",
    "                        self.outs[instance_index].append(np.zeros(self.layer_arch[index]))             \n",
    "                    for index in np.arange(len(self.layer_arch)-1):\n",
    "                        self.outs[instance_index][index+1] = np.append(self.outs[instance_index][index+1],1)\n",
    "\n",
    "                self.deltas = [[np.zeros(self.layer_arch[0]+1)] for ii in self.mini_batch]\n",
    "                for instance_index in np.arange(len(self.mini_batch)):\n",
    "                    for layer_index in np.arange(len(self.layer_arch)-1):\n",
    "                        layer_index += 1\n",
    "                        self.deltas[instance_index].append(np.zeros(self.layer_arch[layer_index]+1))\n",
    "                    self.deltas[instance_index][-1] = self.deltas[instance_index][-1][:-1]\n",
    "                    \n",
    "                # Update outs and weights for each instance in the minibatch\n",
    "                self.update_outs()\n",
    "                self.update_weights(momentum=momentum, learning_rate=learning_rate)\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "        final_outs = [sublist[-1][0] for sublist in self.outs]\n",
    "\n",
    "        if print_training_predictions:\n",
    "            for instance_index in range(len(self.dataset)):\n",
    "                print(f'{self.dataset[instance_index][:-1]}, {final_outs[instance_index]}')\n",
    "            \n",
    "    def predict(self, new_instance):\n",
    "        instance = [new_instance] \n",
    "        for index in np.arange(len(self.layer_arch)):\n",
    "            instance.append(np.zeros(self.layer_arch[index]))             \n",
    "        for index in np.arange(len(self.layer_arch)):\n",
    "            instance[index] = np.append(instance[index],1)\n",
    "    \n",
    "        for layer_index in np.arange(len(self.layer_arch)):\n",
    "            layer_index += 1\n",
    "            weight = self.weights[layer_index-1].T\n",
    "            out = instance[layer_index-1]\n",
    "            new_out_array = np.array(list(map(self.activation_function, weight @ out)))\n",
    "            instance[layer_index][:len(new_out_array)] = new_out_array\n",
    "            \n",
    "        print(f'{new_out_array[0]}')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf65c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the NN can learn the following pattern: x<0.38 or x>0.9=1, \n",
    "# and 0.38<x<0.9=0.3.\n",
    "\n",
    "size_dataset = 1000\n",
    "X = [[np.random.uniform(0, 1)] for ii in np.arange(size_dataset)]\n",
    "y = [0.3 if (feature[0]<0.38 or feature[0]>=0.9) else 1 for feature in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99b9fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst = Multilayer_Perceptron(X, y, [5,5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc95a91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inst.fit(epochs = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25081a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9946149343713591\n"
     ]
    }
   ],
   "source": [
    "inst.predict([0.39])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
